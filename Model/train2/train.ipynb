{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42c605bb-372a-4ad0-a59b-e1c1b6303f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c49e9777-f6d7-461d-b6ef-6098bf1cfd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input shape for images (resize images to 224x224, assuming RGB images)\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 25\n",
    "EPOCHS_INITIAL = 16\n",
    "EPOCHS_FINETUNE = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a39255d9-7b26-46ec-aaed-8f7c11e2fdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use ImageDataGenerator for real-time data augmentation to prevent overfitting\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,               # Normalize the images\n",
    "    rotation_range=20,            # Random rotation\n",
    "    width_shift_range=0.2,        # Random horizontal shift\n",
    "    height_shift_range=0.2,       # Random vertical shift\n",
    "    shear_range=0.2,              # Shear transformation\n",
    "    zoom_range=0.2,               # Zoom in/out\n",
    "    horizontal_flip=True,         # Horizontal flip\n",
    "    fill_mode='nearest'           # Fill mode\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c467d3b-df93-4cf7-bd0d-c568a90ad319",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1./255)  # Only rescale for the test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55ae0818-d2c6-4a99-b682-62df5cf6a9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to your training and test dataset directories\n",
    "train_data_path = 'split/train'\n",
    "test_data_path = 'split/test'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da34a17a-6f54-4775-956c-49b87b95ecd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14551 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "# Loading training data with augmentation\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_path,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical'   # For multi-class classification\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "836976b8-800f-4267-8bce-446faccd2185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3641 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "# Loading test data\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_data_path,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ee4c85b-5c8d-4dd3-9f58-e5b282583b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained EfficientNetB0 model without the top layer\n",
    "base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82307e0c-291f-46d8-8e69-2eddc1668b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze the layers of the base model to prevent training\n",
    "base_model.trainable = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06611f3f-d0a4-4a67-ab61-0353f6d96f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add custom layers for classification\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)  # Pool the features from the base model\n",
    "x = Dense(1024, activation='relu')(x)  # Add a dense layer\n",
    "output = Dense(10, activation='softmax')(x)  # Output layer for 10 classes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a5dd876-e560-499f-958b-084176ba1e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42e76e95-dfe4-49ed-ab65-c3d4d210c87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af2310d9-cb35-4825-8fea-fcc0f0624c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define where to save the best model during training\n",
    "checkpoint_filepath = 'best_model.h5.keras'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "883624aa-48c9-4a5f-b33b-eb2729c5d8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a ModelCheckpoint callback to save the model with the best validation accuracy\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,  # Filepath to save the model\n",
    "    monitor='val_accuracy',        # Monitor validation accuracy\n",
    "    save_best_only=True,           # Save only the best model\n",
    "    mode='max',                    # Maximize the monitored quantity\n",
    "    verbose=1                      # Print saving information to console\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff959e90-124b-4af9-a22a-bd84a5426e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\azcos5\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m454/454\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 637ms/step - accuracy: 0.1092 - loss: 2.3095\n",
      "Epoch 1: val_accuracy improved from -inf to 0.10371, saving model to best_model.h5.keras\n",
      "\u001b[1m454/454\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m353s\u001b[0m 747ms/step - accuracy: 0.1092 - loss: 2.3095 - val_accuracy: 0.1037 - val_loss: 2.3011\n",
      "Epoch 2/10\n",
      "\u001b[1m  1/454\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:27\u001b[0m 325ms/step - accuracy: 0.0938 - loss: 2.3051"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\azcos5\\anaconda3\\Lib\\contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2: val_accuracy improved from 0.10371 to 0.20000, saving model to best_model.h5.keras\n",
      "\u001b[1m454/454\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.0938 - loss: 2.3051 - val_accuracy: 0.2000 - val_loss: 2.2892\n",
      "Epoch 3/10\n",
      "\u001b[1m454/454\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 638ms/step - accuracy: 0.0994 - loss: 2.3011\n",
      "Epoch 3: val_accuracy did not improve from 0.20000\n",
      "\u001b[1m454/454\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m337s\u001b[0m 737ms/step - accuracy: 0.0993 - loss: 2.3011 - val_accuracy: 0.1043 - val_loss: 2.3013\n",
      "Epoch 4/10\n",
      "\u001b[1m  1/454\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:29\u001b[0m 331ms/step - accuracy: 0.1250 - loss: 2.3064\n",
      "Epoch 4: val_accuracy did not improve from 0.20000\n",
      "\u001b[1m454/454\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 738us/step - accuracy: 0.1250 - loss: 2.3064 - val_accuracy: 0.1200 - val_loss: 2.2865\n",
      "Epoch 5/10\n",
      "\u001b[1m454/454\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 636ms/step - accuracy: 0.1024 - loss: 2.3007\n",
      "Epoch 5: val_accuracy did not improve from 0.20000\n",
      "\u001b[1m454/454\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m337s\u001b[0m 735ms/step - accuracy: 0.1024 - loss: 2.3007 - val_accuracy: 0.1048 - val_loss: 2.3008\n",
      "Epoch 6/10\n",
      "\u001b[1m  1/454\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:27\u001b[0m 326ms/step - accuracy: 0.0625 - loss: 2.3027\n",
      "Epoch 6: val_accuracy did not improve from 0.20000\n",
      "\u001b[1m454/454\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 743us/step - accuracy: 0.0625 - loss: 2.3027 - val_accuracy: 0.0400 - val_loss: 2.2951\n",
      "Epoch 7/10\n",
      "\u001b[1m454/454\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 635ms/step - accuracy: 0.1089 - loss: 2.3008\n",
      "Epoch 7: val_accuracy did not improve from 0.20000\n",
      "\u001b[1m454/454\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m336s\u001b[0m 734ms/step - accuracy: 0.1089 - loss: 2.3008 - val_accuracy: 0.1048 - val_loss: 2.3005\n",
      "Epoch 8/10\n",
      "\u001b[1m  1/454\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:30\u001b[0m 333ms/step - accuracy: 0.1250 - loss: 2.2961\n",
      "Epoch 8: val_accuracy did not improve from 0.20000\n",
      "\u001b[1m454/454\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 739us/step - accuracy: 0.1250 - loss: 2.2961 - val_accuracy: 0.0400 - val_loss: 2.3064\n",
      "Epoch 9/10\n",
      "\u001b[1m454/454\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 638ms/step - accuracy: 0.1020 - loss: 2.3014\n",
      "Epoch 9: val_accuracy did not improve from 0.20000\n",
      "\u001b[1m454/454\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m337s\u001b[0m 737ms/step - accuracy: 0.1020 - loss: 2.3014 - val_accuracy: 0.1043 - val_loss: 2.3005\n",
      "Epoch 10/10\n",
      "\u001b[1m  1/454\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:30\u001b[0m 332ms/step - accuracy: 0.1250 - loss: 2.2954\n",
      "Epoch 10: val_accuracy did not improve from 0.20000\n",
      "\u001b[1m454/454\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 761us/step - accuracy: 0.1250 - loss: 2.2954 - val_accuracy: 0.1200 - val_loss: 2.3014\n"
     ]
    }
   ],
   "source": [
    "steps_per_epoch = train_generator.samples // BATCH_SIZE\n",
    "validation_steps = test_generator.samples // BATCH_SIZE\n",
    "\n",
    "# Ensure that steps_per_epoch is at least 1\n",
    "if steps_per_epoch == 0:\n",
    "    steps_per_epoch = 1\n",
    "\n",
    "if validation_steps == 0:\n",
    "    validation_steps = 1\n",
    "\n",
    "# Train the model with the ModelCheckpoint callback\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // BATCH_SIZE,\n",
    "    epochs=EPOCHS_INITIAL,  # Initial training epochs\n",
    "    validation_data=test_generator,\n",
    "    validation_steps=test_generator.samples // BATCH_SIZE,\n",
    "    callbacks=[checkpoint]  # Add the checkpoint callback here\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8ca8b8d2-33be-4fb9-a682-a4a64de8be2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze some layers of the base model for fine-tuning\n",
    "base_model.trainable = True\n",
    "fine_tune_at = len(base_model.layers) // 2  # Unfreeze half of the layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e8c9b23c-74a1-4e7f-b3f6-9d29c4c33176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recompile with a lower learning rate for fine-tuning\n",
    "model.compile(optimizer=Adam(learning_rate=1e-5), \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216ee775-5bb6-46f1-8677-480cc409a176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m454/454\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.2692 - loss: 2.1146\n",
      "Epoch 1: val_accuracy did not improve from 0.20000\n",
      "\u001b[1m454/454\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1018s\u001b[0m 2s/step - accuracy: 0.2695 - loss: 2.1139 - val_accuracy: 0.0650 - val_loss: 2.3467\n",
      "Epoch 2/5\n",
      "\u001b[1m  1/454\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15:05\u001b[0m 2s/step - accuracy: 0.4062 - loss: 1.5898\n",
      "Epoch 2: val_accuracy did not improve from 0.20000\n",
      "\u001b[1m454/454\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.4062 - loss: 1.5898 - val_accuracy: 0.0400 - val_loss: 2.3660\n",
      "Epoch 3/5\n",
      "\u001b[1m454/454\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.5032 - loss: 1.3410\n",
      "Epoch 3: val_accuracy improved from 0.20000 to 0.53733, saving model to best_model.h5.keras\n",
      "\u001b[1m454/454\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m970s\u001b[0m 2s/step - accuracy: 0.5032 - loss: 1.3409 - val_accuracy: 0.5373 - val_loss: 1.2319\n",
      "Epoch 4/5\n",
      "\u001b[1m  1/454\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15:04\u001b[0m 2s/step - accuracy: 0.5000 - loss: 1.2152\n",
      "Epoch 4: val_accuracy did not improve from 0.53733\n",
      "\u001b[1m454/454\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 765us/step - accuracy: 0.5000 - loss: 1.2152 - val_accuracy: 0.5200 - val_loss: 1.2626\n",
      "Epoch 5/5\n",
      "\u001b[1m454/454\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.5571 - loss: 1.1741"
     ]
    }
   ],
   "source": [
    "# Fine-tune the model\n",
    "fine_tune_history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // BATCH_SIZE,\n",
    "    epochs=EPOCHS_FINETUNE,  # Fine-tuning epochs\n",
    "    validation_data=test_generator,\n",
    "    validation_steps=test_generator.samples // BATCH_SIZE,\n",
    "    callbacks=[checkpoint]  # Continue saving the best model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63688ef7-e83d-4a67-a4e4-7bde7a58d688",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Display validation accuracy for both initial training and fine-tuning\n",
    "val_acc_initial = history.history['val_accuracy']\n",
    "val_acc_finetune = fine_tune_history.history['val_accuracy']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2bf52e-9ec6-41c3-a064-1ad2c50f6b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final validation accuracy after fine-tuning\n",
    "final_val_acc = val_acc_finetune[-1]\n",
    "print(f\"Final validation accuracy after fine-tuning: {final_val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d76a6c-901f-4651-beda-e0ff2007bf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Visualize validation accuracy over epochs\n",
    "epochs_initial = range(1, len(val_acc_initial) + 1)\n",
    "epochs_finetune = range(len(val_acc_initial) + 1, len(val_acc_initial) + len(val_acc_finetune) + 1)\n",
    "\n",
    "plt.plot(epochs_initial, val_acc_initial, label='Initial Training Val Accuracy')\n",
    "plt.plot(epochs_finetune, val_acc_finetune, label='Fine-Tuning Val Accuracy')\n",
    "plt.title('Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854bc0fd-2c7a-4270-8e6e-98dd94328c0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
